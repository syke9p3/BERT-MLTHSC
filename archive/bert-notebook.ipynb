{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = \"gklmip/bert-tagalog-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = pd.read_csv('./dataset/cleaned_mlthsc.csv', nrows=1000)\n",
    "\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [\"Age\", \"Gender\", \"Physical\", \"Race\", \"Religion\", \"Others\"]\n",
    "id2label = {idx:label for idx, label in enumerate(LABELS)}\n",
    "label2id = {label:idx for idx, label in enumerate(LABELS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    text = data[\"Text\"]\n",
    "\n",
    "    encoding = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "    \n",
    "    labels = data[LABELS]\n",
    "    \n",
    "    representation = {\n",
    "        'input_ids': encoding['input_ids'].flatten(),\n",
    "        'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        'labels': torch.FloatTensor(labels)\n",
    "    }\n",
    "\n",
    "    return representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 800\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 200\n",
      "})\n",
      "{'input_ids': [101, 1767, 23941, 110, 160, 43209, 5230, 1779, 1767, 15721, 5489, 1109, 8774, 25301, 1754, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [0.0, 0.0, 1.0, 1.0, 0.0, 0.0]}\n",
      "{'input_ids': [101, 18215, 6067, 1116, 28888, 3591, 1863, 3587, 36297, 2711, 14683, 1744, 1894, 12936, 19451, 1741, 5397, 2309, 3854, 1894, 3407, 51114, 1894, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# Create a list of encoded examples for train and test data\n",
    "encoded_train_data = [preprocess_data(row) for _, row in train_data.iterrows()]\n",
    "encoded_test_data = [preprocess_data(row) for _, row in test_data.iterrows()]\n",
    "\n",
    "# Combine the encoded examples into a dictionary\n",
    "encoded_train_dict = {key: [example[key] for example in encoded_train_data] for key in encoded_train_data[0]}\n",
    "encoded_test_dict = {key: [example[key] for example in encoded_test_data] for key in encoded_test_data[0]}\n",
    "\n",
    "# Convert the dictionaries to datasets\n",
    "train_dataset = Dataset.from_dict(encoded_train_dict)\n",
    "test_dataset = Dataset.from_dict(encoded_test_dict)\n",
    "\n",
    "# Print the first few examples to verify the encoding\n",
    "print(train_dataset)\n",
    "print(test_dataset)\n",
    "\n",
    "print(train_dataset[0])\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at gklmip/bert-tagalog-base-uncased and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(bert_model,\n",
    "                                                           problem_type=\"multi_label_classification\",\n",
    "                                                           num_labels=len(LABELS),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "metric_name = \"hamming_loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    \"checkpoint\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from transformers import EvalPrediction\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EvalPrediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Repo\\Python\\BERT-MLTHSC\\d-try.ipynb Cell 11\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repo/Python/BERT-MLTHSC/d-try.ipynb#X13sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     label_metrics[\u001b[39m'\u001b[39m\u001b[39mHamming Loss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m hamming_loss\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repo/Python/BERT-MLTHSC/d-try.ipynb#X13sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m label_metrics\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Repo/Python/BERT-MLTHSC/d-try.ipynb#X13sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_metrics\u001b[39m(p: EvalPrediction):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repo/Python/BERT-MLTHSC/d-try.ipynb#X13sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     preds \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39mpredictions[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(p\u001b[39m.\u001b[39mpredictions, \u001b[39mtuple\u001b[39m) \u001b[39melse\u001b[39;00m p\u001b[39m.\u001b[39mpredictions\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repo/Python/BERT-MLTHSC/d-try.ipynb#X13sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mpreds\u001b[39m\u001b[39m\"\u001b[39m, preds)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'EvalPrediction' is not defined"
     ]
    }
   ],
   "source": [
    "def multilabel_metrics(predictions, labels, threshold=0.5):\n",
    "\n",
    "    print(\"predictions:\", predictions)\n",
    "\n",
    "    # Apply sigmoid activation to logits/raw scores from the classifier \n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probabilities = sigmoid(torch.Tensor(predictions))\n",
    "\n",
    "    print(\"probabilities:\", probabilities)\n",
    "\n",
    "    # Filter out labels using the 0.5 threshold\n",
    "    y_pred = np.zeros(probabilities.shape)\n",
    "    y_pred[np.where(probabilities >= threshold)] = 1\n",
    "\n",
    "    y_true = np.zeros(labels.shape)\n",
    "    y_true[np.where(labels == 1)] = 1\n",
    "\n",
    "    print(\"Y PRED:\", y_pred)\n",
    "    print(\"Y TRUE:\", y_true)\n",
    "    \n",
    "    confusion_matrix = multilabel_confusion_matrix(y_true, y_pred)\n",
    "    print(confusion_matrix)\n",
    "    label_metrics = {}\n",
    "    \n",
    "    classes = ['Age', 'Gender', 'Physical', 'Race', 'Religion', 'Others']\n",
    "\n",
    "    for i in range(confusion_matrix.shape[0]):\n",
    "        TP = confusion_matrix[i, 1, 1]  # True Positives\n",
    "        FP = confusion_matrix[i, 0, 1]  # False Positives\n",
    "        FN = confusion_matrix[i, 1, 0]  # False Negatives\n",
    "        TN = confusion_matrix[i, 0, 0]  # True Negatives\n",
    "\n",
    "        # TN FP\n",
    "        # FN TP \n",
    "\n",
    "        precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "        recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "        f1_score = (2 * precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "        label_name = classes[i]\n",
    "\n",
    "        label_metrics[label_name] = {\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1-Score\": f1_score\n",
    "        }\n",
    "\n",
    "    # Calculate Hamming Loss\n",
    "    xor_result = np.logical_xor(y_true, y_pred)\n",
    "    xor_sum = np.sum(xor_result)\n",
    "    hamming_loss = xor_sum / (y_true.shape[0] * y_true.shape[1])\n",
    "    \n",
    "    label_metrics['Hamming Loss'] = hamming_loss\n",
    "\n",
    "    return label_metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "\n",
    "    print(\"preds\", preds)\n",
    "\n",
    "    result = multilabel_metrics(predictions=preds, labels=p.label_ids, threshold=0.5)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Repo\\Python\\BERT-MLTHSC\\d-try.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Repo/Python/BERT-MLTHSC/d-try.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repo/Python/BERT-MLTHSC/d-try.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     model,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repo/Python/BERT-MLTHSC/d-try.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     args,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repo/Python/BERT-MLTHSC/d-try.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39mtrain_dataset,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repo/Python/BERT-MLTHSC/d-try.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39mtest_dataset,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repo/Python/BERT-MLTHSC/d-try.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repo/Python/BERT-MLTHSC/d-try.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repo/Python/BERT-MLTHSC/d-try.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2]\n",
      " [1 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 2, 1, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n",
    "(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Repo\\Python\\BERT-MLTHSC\\venv\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/125 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  2%|▏         | 2/125 [03:03<3:16:26, 95.82s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to interrupt the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'venv (Python 3.11.2)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"model-trial-1\"\n",
    "trained_model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                                                        939\n",
       "Text        TANGINA! LAGI NALANG NADADAMAY YUNG LAG LILING...\n",
       "Age                                                         0\n",
       "Gender                                                      0\n",
       "Physical                                                    0\n",
       "Race                                                        1\n",
       "Religion                                                    1\n",
       "Others                                                      0\n",
       "Name: 938, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 14\n",
    "row = test_data.iloc[index]\n",
    "test_sentence = row['Text']\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors='pt')\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_test_sentence = preprocess_text(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.9301, -3.2368, -2.9677,  2.2064,  1.7061, -3.6536]]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model_outputs = trained_model(**encoded_test_sentence)\n",
    "\n",
    "print(model_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.050687409937381744, 0.03780565410852432, 0.04890599846839905, 0.9008221626281738, 0.8463234901428223, 0.02524476684629917]\n"
     ]
    }
   ],
   "source": [
    "predictions = model_outputs.logits.sigmoid().tolist()[0]  # Apply sigmoid to get probabilities\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: TANGINA! LAGI NALANG NADADAMAY YUNG LAG LILINGKOD KO AH? TSK PAKITANG TAO? NAKAKA PUNYETA TALAGA MGA PINAG SASBI MK EH NOH PALIBHASA MUSLIM MUSLIMAN MA KAYA KA GANYAN\n",
      "Labels:\n",
      "(Race, 90.08%)\n",
      "(Religion, 84.63%)\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "predicted_labels = [(label, f\"{pred*100:.2f}%\") for label, pred in zip(LABELS, predictions) if pred >= threshold]\n",
    "print(\"Input:\", test_sentence)\n",
    "print(\"Labels:\")\n",
    "for label, probability in predicted_labels:\n",
    "    print(f\"({label}, {probability})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on test sentence\n",
    "\n",
    "Palagay ng bagong test_sentence para matry kung tama ang labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ambobo ng mga batang katoliko na bisaya\n",
      "Probabilities:  [('Age', '50.16%'), ('Gender', '6.47%'), ('Physical', '5.72%'), ('Race', '29.04%'), ('Religion', '89.39%'), ('Others', '1.56%')]\n",
      "Labels:\n",
      "(Age, 50.16%)\n",
      "(Religion, 89.39%)\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"ambobo ng mga batang katoliko na bisaya\"\n",
    "\n",
    "encoded_test_sentence = preprocess_text(test_sentence)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_outputs = trained_model(**encoded_test_sentence)\n",
    "\n",
    "predictions = model_outputs.logits.sigmoid().tolist()[0]  # Apply sigmoid to get probabilities\n",
    "label_probabilities = [(label, f\"{prob * 100:.2f}%\") for label, prob in zip(LABELS, predictions)]\n",
    "\n",
    "threshold = 0.5\n",
    "predicted_labels = [(label, f\"{pred*100:.2f}%\") for label, pred in zip(LABELS, predictions) if pred >= threshold]\n",
    "print(\"Input:\", test_sentence)\n",
    "print(\"Probabilities: \", label_probabilities)\n",
    "\n",
    "print(\"Labels:\")\n",
    "for label, probability in predicted_labels:\n",
    "    print(f\"({label}, {probability})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

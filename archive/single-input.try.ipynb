{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Denise\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Denise\\Documents\\GitHub\\BERT-MLTHSC\\learning_materials\\single-input.try.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Denise/Documents/GitHub/BERT-MLTHSC/learning_materials/single-input.try.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Denise/Documents/GitHub/BERT-MLTHSC/learning_materials/single-input.try.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Denise/Documents/GitHub/BERT-MLTHSC/learning_materials/single-input.try.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at gklmip/bert-tagalog-base-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gklmip/bert-tagalog-base-uncased\"\n",
    "BERT_MODEL = AutoModel.from_pretrained(model_name, return_dict=True)\n",
    "BERT_TOKENIZER = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './dataset/train-try.csv'\n",
    "val_path = './dataset/val-try.csv'\n",
    "test_path = './dataset/test-try.csv'\n",
    "dataset_path = './dataset/mlthsc.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dataset_path)\n",
    "train_df = pd.read_csv(train_path)\n",
    "val_df = pd.read_csv(val_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "LABELS = ['Age', 'Gender', 'Physical', 'Race', 'Religion', 'Others']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating the flow of a single input to BERT output\n",
    "\n",
    "Kuha tayo ng isang sample text mula sa dataset natin (index 10 sa mlthsc.csv)\n",
    "\n",
    "tas kunin yung representation as input to BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_row = df.iloc[10]\n",
    "\n",
    "hate_speech_text = data_row['Text']\n",
    "labels = data_row[LABELS]\n",
    "\n",
    "\n",
    "encoding = BERT_TOKENIZER.encode_plus(\n",
    "    hate_speech_text,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors='pt',\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True\n",
    ")\n",
    "\n",
    "\n",
    "representation = {\n",
    "    'input_ids': encoding['input_ids'].flatten(),\n",
    "    'attention_mask': encoding['attention_mask'].flatten(),\n",
    "    'labels': torch.FloatTensor(labels)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "101 = [CLS]\n",
    "18215 = ng\n",
    "2229 = mga\n",
    "102 = [SEP]\n",
    "0 = [PAD]\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tangina ng mga girl gamer sa ml ambobobo nyo mag ros nalang kayo\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 18215,  2229,  1745,  1762,  3780,  3629,  1122,  1741, 13305,\n",
       "         33900,  2573,  2573,  5193,  1814,  3438,  8288,  2921,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(hate_speech_text)\n",
    "encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output ng BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.8359,  0.4488,  0.9444,  ..., -0.7361, -0.0301, -0.4596],\n",
       "         [-0.2004, -1.6600, -0.0554,  ..., -0.2445,  1.3696,  0.6472],\n",
       "         [-0.0274, -1.0908, -0.5952,  ..., -0.5364,  0.5854, -0.1231],\n",
       "         ...,\n",
       "         [ 0.1735, -0.3333, -0.1038,  ...,  0.2210,  0.5366,  0.2195],\n",
       "         [ 0.6714, -0.5266, -0.3117,  ...,  0.5718, -0.7893,  0.4809],\n",
       "         [ 0.4980, -0.0049,  0.6744,  ...,  0.1516, -0.6812,  0.7967]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.2852, -0.0088, -0.4217,  0.5029, -0.5775,  0.8261, -0.3018,  0.3172,\n",
       "          0.8902, -0.3816,  0.6675, -0.4433, -0.1287,  0.0369, -0.3644,  0.2761,\n",
       "         -0.3168, -0.7029, -0.5461, -0.6946, -0.8801, -0.3294, -0.4730, -0.1471,\n",
       "         -0.3901,  0.6406,  0.4061, -0.5049,  0.3023, -0.0564,  0.3540, -0.0063,\n",
       "         -0.3668,  0.5559,  0.0557,  0.4991,  0.1714, -0.5220,  0.4974,  0.1893,\n",
       "         -0.6577,  0.4095, -0.2713,  0.7381, -0.5780,  0.4668,  0.8155,  0.3512,\n",
       "         -0.0937,  0.1215,  0.0155,  0.4564, -0.8095,  0.2691, -0.5942,  0.0905,\n",
       "         -0.2295,  0.2933,  0.4473, -0.4428, -0.6329,  0.0458, -0.3906,  0.1685,\n",
       "         -0.0287, -0.0474, -0.6212,  0.0337,  0.6727,  0.1221,  0.1702,  0.2654,\n",
       "          0.2230, -0.1879,  0.8278,  0.0011,  0.1178,  0.7381, -0.4318,  0.1984,\n",
       "          0.3492,  0.2495,  0.7717, -0.5348, -0.0374,  0.1492, -0.1369,  0.4636,\n",
       "         -0.2223, -0.2904, -0.5011,  0.9459,  0.0831, -0.1460,  0.6022, -0.4318,\n",
       "         -0.8541, -0.3106, -0.2048, -0.3424,  0.5629,  0.2388,  0.0582, -0.6813,\n",
       "         -0.4817, -0.1476, -0.0969, -0.1829, -0.1092, -0.0205, -0.4416,  0.8396,\n",
       "         -0.2977,  0.4490,  0.3818, -0.1213,  0.2199, -0.3514, -0.4634, -0.5525,\n",
       "          0.0138, -0.7183,  0.5295,  0.0631, -0.3212, -0.4714, -0.5062, -0.3818,\n",
       "         -0.0882,  0.1716, -0.3377, -0.3358, -0.6731, -0.5121, -0.5822, -0.2529,\n",
       "         -0.0852,  0.5504,  0.5395,  0.6051, -0.4678,  0.1027, -0.0175, -0.2380,\n",
       "         -0.6727,  0.6031, -0.5218, -0.0694,  0.6240,  0.0933, -0.6606,  0.1593,\n",
       "         -0.3654, -0.0269, -0.0314,  0.1709,  0.3120, -0.1549,  0.3673,  0.1895,\n",
       "         -0.0482,  0.5984,  0.2405,  0.4383, -0.7569, -0.4066, -0.8399,  0.0157,\n",
       "          0.6212,  0.1411, -0.0948,  0.2422, -0.6122,  0.0127,  0.3622,  0.5735,\n",
       "          0.1722,  0.7447,  0.5579,  0.0521, -0.8055,  0.6261, -0.4854, -0.3685,\n",
       "         -0.6568,  0.7461, -0.7637, -0.1851,  0.5016, -0.5267,  0.2449, -0.1608,\n",
       "         -0.3126,  0.1173,  0.1798,  0.3194, -0.3375,  0.7606, -0.0880,  0.2780,\n",
       "          0.1724, -0.0562,  0.5295,  0.6508, -0.3974, -0.6695,  0.0622,  0.1628,\n",
       "          0.6604, -0.2037, -0.1872,  0.2016,  0.5692,  0.3147,  0.5026,  0.1630,\n",
       "         -0.5488,  0.4222, -0.3936,  0.5382,  0.4974,  0.0326,  0.2599, -0.5032,\n",
       "          0.0408, -0.3365, -0.3448, -0.0134,  0.9230, -0.0928, -0.5452,  0.8212,\n",
       "          0.2087,  0.3008,  0.0344, -0.2168,  0.4606, -0.5354,  0.5077,  0.2184,\n",
       "         -0.1652, -0.4769,  0.1628,  0.4813, -0.4470, -0.3997,  0.0846, -0.5984,\n",
       "         -0.2632,  0.4819,  0.5230, -0.5715,  0.1618, -0.0649,  0.5446,  0.5587,\n",
       "         -0.5276, -0.0333,  0.0702, -0.3121, -0.4423,  0.2618,  0.2213, -0.0239,\n",
       "         -0.1203, -0.6509,  0.4956, -0.1021, -0.4379,  0.1202, -0.0629,  0.3241,\n",
       "          0.0906,  0.2782,  0.1423, -0.1174,  0.4733,  0.5579,  0.5673, -0.3308,\n",
       "         -0.6194, -0.1775, -0.7540,  0.6242,  0.2168,  0.1080, -0.2324, -0.2927,\n",
       "         -0.2111,  0.4660, -0.2246, -0.5008, -0.3779, -0.5678,  0.6558, -0.1220,\n",
       "          0.5634,  0.1567, -0.3910, -0.1126, -0.4361,  0.4119, -0.4360,  0.4965,\n",
       "          0.3489,  0.3319,  0.2409, -0.2430, -0.7359,  0.1681,  0.7064,  0.6043,\n",
       "          0.0933, -0.4687, -0.1943, -0.3103, -0.7083, -0.2759, -0.5368, -0.3142,\n",
       "          0.3298,  0.0599,  0.1980, -0.2540, -0.5942,  0.4662, -0.0460,  0.1283,\n",
       "         -0.3143, -0.1086,  0.4702, -0.7106, -0.1120,  0.4737, -0.3413,  0.3818,\n",
       "         -0.6507,  0.8508,  0.5364,  0.6234, -0.2056, -0.1892,  0.0590, -0.0749,\n",
       "          0.3714, -0.2789,  0.2108,  0.2717,  0.3239,  0.1823, -0.2500,  0.3007,\n",
       "         -0.8403,  0.3617,  0.5590,  0.0947, -0.5564, -0.4952,  0.2871, -0.0312,\n",
       "          0.4054,  0.5870,  0.4880, -0.2018,  0.0266,  0.2223, -0.6585,  0.2415,\n",
       "          0.3650,  0.1978,  0.1023,  0.1150, -0.7115,  0.6421,  0.1627,  0.1546,\n",
       "          0.4847,  0.0597, -0.0274, -0.2946, -0.7953,  0.6609,  0.0517,  0.1677,\n",
       "          0.6842, -0.2862,  0.6564,  0.5275,  0.5030, -0.5935, -0.1999,  0.5570,\n",
       "          0.3378, -0.4404,  0.0472, -0.5199,  0.1989,  0.4118,  0.1483,  0.2921,\n",
       "          0.4399,  0.5408,  0.1811, -0.3955,  0.2426,  0.2631, -0.2913, -0.0940,\n",
       "          0.0339, -0.5203,  0.3081,  0.2269, -0.0877, -0.5377, -0.2116,  0.1524,\n",
       "          0.6162, -0.2957, -0.6338,  0.0257, -0.4348, -0.3229, -0.8300,  0.5457,\n",
       "          0.5594,  0.1312,  0.2523,  0.5814, -0.3407, -0.1055, -0.5955,  0.6858,\n",
       "         -0.0340,  0.3050, -0.6830,  0.2035, -0.4701, -0.2485, -0.3385,  0.1339,\n",
       "         -0.7328,  0.0055,  0.3845,  0.3606, -0.3792, -0.2126, -0.3845,  0.7102,\n",
       "         -0.2599,  0.1391,  0.3295, -0.7563, -0.3745, -0.2878,  0.0095, -0.6608,\n",
       "          0.0221,  0.0346,  0.2438,  0.0127, -0.3990,  0.0780,  0.5288,  0.3397,\n",
       "          0.1925, -0.0666,  0.6324, -0.6536,  0.2073,  0.4268,  0.0782, -0.1334,\n",
       "          0.8864,  0.4530,  0.4314,  0.6282,  0.3183, -0.1859,  0.5849, -0.5663,\n",
       "          0.1066,  0.8891, -0.0115,  0.5752,  0.7784, -0.4250,  0.2710,  0.4701,\n",
       "         -0.4608, -0.1763,  0.0890,  0.4869, -0.2741, -0.5809, -0.2460, -0.3180,\n",
       "         -0.4842,  0.3401, -0.1566, -0.6262,  0.0904, -0.6047,  0.0849, -0.3103,\n",
       "         -0.4612,  0.2283, -0.6399, -0.0195, -0.3729,  0.3963,  0.3110,  0.0076,\n",
       "          0.1800,  0.4367, -0.2311, -0.8859, -0.2567,  0.0598, -0.2085, -0.5049,\n",
       "          0.7162, -0.6917,  0.2788, -0.0748, -0.7375,  0.5900,  0.1822, -0.3023,\n",
       "          0.4685, -0.5663, -0.0820,  0.4807,  0.1209, -0.0276,  0.2349, -0.2525,\n",
       "          0.0801, -0.2967,  0.4615, -0.2880, -0.3406, -0.4526,  0.5113, -0.1977,\n",
       "         -0.4623,  0.5269,  0.7358,  0.6683, -0.6713,  0.1804,  0.5699,  0.3323,\n",
       "          0.0577, -0.1676, -0.4775, -0.0486,  0.2368,  0.3710,  0.2411,  0.6524,\n",
       "         -0.3652,  0.4013, -0.8547,  0.5546,  0.0401,  0.1199,  0.3902,  0.4173,\n",
       "          0.0440, -0.6465, -0.3509, -0.1966,  0.4688,  0.7379,  0.3626, -0.1117,\n",
       "         -0.3837, -0.3433, -0.6024, -0.0357,  0.2985, -0.6985,  0.4883, -0.3106,\n",
       "          0.7137,  0.2002, -0.3984, -0.7327, -0.6555, -0.6303, -0.3212, -0.0774,\n",
       "          0.5364, -0.5396, -0.2957, -0.3042,  0.5075,  0.1565,  0.0737, -0.3970,\n",
       "         -0.4548,  0.2185, -0.2810,  0.2065, -0.0994, -0.1674, -0.1349,  0.3506,\n",
       "          0.3013, -0.6130,  0.5481,  0.6125,  0.4117,  0.3647, -0.0114,  0.1800,\n",
       "          0.4778, -0.0578,  0.6347, -0.4202,  0.5929,  0.6287, -0.2134,  0.2617,\n",
       "         -0.6121,  0.4388,  0.5402,  0.3005, -0.0886, -0.0441, -0.2580, -0.2756,\n",
       "         -0.5223, -0.0622,  0.6294, -0.5563, -0.4126, -0.0158, -0.7092,  0.2670,\n",
       "         -0.7136,  0.4329, -0.6329, -0.1556,  0.2512,  0.5178,  0.0026, -0.0217,\n",
       "         -0.2650,  0.0404,  0.4049,  0.0659,  0.0979,  0.0150,  0.1482,  0.0221,\n",
       "          0.2483, -0.3678,  0.6197, -0.3590,  0.5792,  0.5041, -0.4193,  0.0217,\n",
       "         -0.0678, -0.4322, -0.3906,  0.6901, -0.6300,  0.6618,  0.3704,  0.0057,\n",
       "         -0.4393, -0.2730,  0.5107, -0.4530, -0.4340, -0.6880,  0.0872, -0.3297,\n",
       "          0.0049, -0.0855, -0.0692, -0.3004, -0.4089,  0.4247,  0.5964,  0.2517,\n",
       "         -0.1346, -0.3064, -0.3708,  0.0466, -0.7354, -0.2926, -0.3059, -0.8243,\n",
       "         -0.6185,  0.5553, -0.0376,  0.1016, -0.0564, -0.7429,  0.1675, -0.1941,\n",
       "          0.4274, -0.0034, -0.5123,  0.1529,  0.1038, -0.3272, -0.1798,  0.6670,\n",
       "          0.0228, -0.4975, -0.2666, -0.1080, -0.2810, -0.1636,  0.3069,  0.8333,\n",
       "         -0.0546,  0.1834,  0.7537, -0.5809,  0.0163, -0.6012,  0.1491,  0.4249,\n",
       "          0.3633, -0.0735,  0.5954,  0.7716,  0.0257,  0.1112,  0.1627, -0.6610,\n",
       "         -0.1773,  0.7260,  0.3864,  0.0333,  0.4030, -0.0298,  0.3919, -0.0635,\n",
       "          0.1357,  0.0961,  0.2716,  0.7513, -0.1564, -0.5459, -0.5650,  0.1366,\n",
       "         -0.4756,  0.3369, -0.5016, -0.5869, -0.2339, -0.2650,  0.3153, -0.2253,\n",
       "          0.4047, -0.1368, -0.1366,  0.4624,  0.3579,  0.1228, -0.5344, -0.1079]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']\n",
    "\n",
    "output = BERT_MODEL(input_ids, attention_mask=attention_mask)\n",
    "output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [CLS] Token\n",
    "\n",
    "*last_hidden_state* nagrerepresent ng embedding ng CLS token, which is a tensor of shape (768) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.3589e-01,  4.4881e-01,  9.4441e-01,  7.4970e-01,  1.0030e+00,\n",
       "        -1.9352e+00,  3.1232e-01, -1.1254e-01,  3.8781e-01,  4.1114e-01,\n",
       "        -9.6400e-01,  9.4284e-01,  2.3553e-01,  1.3223e+00,  4.3053e-01,\n",
       "        -6.5761e-01,  3.9198e-01,  7.7849e-02,  2.6433e-01,  8.9570e-01,\n",
       "        -1.0710e+00, -2.1475e+00,  7.6227e-01,  6.0151e-01,  7.9761e+00,\n",
       "        -2.6235e-01, -1.7995e+00,  8.6282e-01,  8.0839e-01, -1.9218e-01,\n",
       "         2.2226e-02,  2.0101e+00, -1.3445e+00, -9.7304e-02,  7.2417e-01,\n",
       "        -2.2077e-01,  5.6884e-01,  3.3936e-01, -1.1537e+00,  3.0945e-01,\n",
       "        -8.8095e-01,  8.8641e-01, -4.0872e-01,  8.1410e-01,  2.4943e-01,\n",
       "        -1.5284e+00, -1.5121e-01,  7.4530e-01, -1.2743e-01,  8.8579e-01,\n",
       "        -8.6979e-01,  7.8161e-01, -3.3494e-01, -2.1311e+00, -5.4304e-01,\n",
       "        -6.4087e-01, -1.7144e-01,  6.6150e-02,  8.4312e-01,  2.2235e-01,\n",
       "         3.7095e-01, -6.9918e-01,  1.6027e-01, -1.3652e+00,  3.7835e-01,\n",
       "        -8.8689e-01,  6.7411e-01,  1.1421e-01, -6.6348e-01, -8.3097e-01,\n",
       "        -5.3201e-01,  8.6465e-01,  1.0375e+00, -1.3082e+00, -1.7483e+00,\n",
       "        -2.2859e+00, -1.9491e-01, -3.6335e-01,  1.0081e-01, -3.1252e-01,\n",
       "        -9.8692e-02, -9.5653e-01, -2.0840e+00,  4.5647e-01,  4.2393e-01,\n",
       "        -1.5234e-01,  7.4375e-02, -2.6652e-01,  2.3836e-01, -3.7104e-01,\n",
       "        -5.2039e-01,  3.6970e-01,  1.0709e-01,  1.5988e+00,  4.6316e-01,\n",
       "        -6.3237e-01, -1.6876e-01, -1.2647e+00,  1.1029e-02, -2.4696e-01,\n",
       "         4.9814e-01,  1.1028e+00,  2.3611e-01, -3.1844e-01,  6.9067e-01,\n",
       "        -4.6153e-01,  1.7965e+00,  2.2558e-02,  9.2730e-01,  3.7335e-01,\n",
       "         7.4651e-01, -9.9675e-01,  5.4388e-01,  4.4434e-01,  9.1711e-01,\n",
       "         1.1428e+00, -1.1156e+00,  3.7740e-01,  6.0732e+00, -8.7569e-01,\n",
       "         4.3845e-02,  1.6446e-01, -1.6875e+00,  1.9954e-01,  3.4687e-01,\n",
       "         6.4414e-01,  8.7566e-01, -2.4874e-01,  3.9347e-01,  5.0172e-01,\n",
       "         5.5766e-01, -9.0211e-01, -3.1458e-01, -6.5072e-01, -7.0994e-01,\n",
       "        -5.3144e-01,  6.5772e-01, -1.1712e+00, -8.5525e-02, -8.5516e-01,\n",
       "        -1.1796e+00,  1.5499e+00,  5.2049e-01,  2.0878e-01, -1.6118e-01,\n",
       "        -5.4256e-01, -2.9962e-01, -1.1844e+00, -5.5786e-01, -7.9897e-01,\n",
       "        -1.7933e+00,  1.0710e+00, -8.1234e-01,  4.9186e-01,  3.3551e-01,\n",
       "         1.6863e-01,  6.4261e-01,  3.0886e-01, -1.5910e-01, -7.6881e-01,\n",
       "         7.6412e-01,  2.7998e-01, -1.9508e-01,  5.6393e-01, -2.8854e-01,\n",
       "         4.6883e-01,  4.9063e-01, -3.2525e-01, -8.8465e-01,  1.0574e+00,\n",
       "         4.8996e-01, -1.1522e+00, -1.7235e+00,  2.0129e-01, -1.5992e-01,\n",
       "        -5.8544e-01, -9.9020e-02,  2.2202e-02, -6.0686e-03, -1.4884e+00,\n",
       "         1.3472e+00,  2.2196e+00, -6.3937e-01,  1.4239e-01, -5.8149e-02,\n",
       "         7.0698e-01,  1.8414e-01,  5.1477e-01, -1.7999e+00, -7.7096e-01,\n",
       "        -6.4642e-01, -6.1606e-03,  2.1473e-01, -4.9228e-01,  5.6985e-01,\n",
       "         5.9218e-01, -2.6537e-01, -1.2542e-01,  1.5516e-01,  2.0968e-01,\n",
       "        -2.1867e-01,  1.4973e-01,  1.1957e-01, -8.8072e-01,  2.7651e-01,\n",
       "        -2.7905e-01,  1.5829e+00, -1.5269e+00,  2.1875e-01,  6.6555e-02,\n",
       "        -1.5953e+00, -1.1383e+00,  1.2713e+00, -3.8652e-01,  1.0252e+00,\n",
       "         4.5317e-01,  9.0347e-01,  3.7609e-01,  8.1481e-01,  6.1189e-01,\n",
       "        -1.8468e+00,  5.6005e-01, -2.8796e-01, -6.4397e-02, -1.0272e+00,\n",
       "         2.4610e-01, -1.8722e+00,  9.2846e-01,  2.0306e-01,  1.4885e-01,\n",
       "        -1.0916e+00,  1.0245e-02,  6.8055e-01,  7.2778e-02,  1.4815e-01,\n",
       "         1.3961e-01,  2.4030e-01,  3.3920e-01, -1.0383e-01, -1.5483e+00,\n",
       "         7.9274e-01,  7.5967e-01, -1.6812e+00, -1.0626e+00, -2.6769e-01,\n",
       "        -6.8453e-01, -1.1439e+00, -1.3023e+00, -1.7910e+00, -5.6719e-01,\n",
       "        -1.0567e+00, -4.8909e-01, -6.8535e-01, -3.7495e-01,  9.0778e-01,\n",
       "        -7.3210e-01,  4.0539e-01, -4.5634e-01, -1.0544e-01, -1.2724e+00,\n",
       "         5.8287e-01,  1.6475e-01, -9.6806e-01,  7.9422e-01,  9.9135e-01,\n",
       "         1.2966e+00, -1.3479e+00,  7.5638e-02, -1.0275e+00, -9.3885e-01,\n",
       "         1.2817e-01, -8.1381e-01,  2.6439e-01,  5.7231e-02,  2.6931e-02,\n",
       "         1.5699e-01, -1.0292e+00, -4.7023e-01, -5.5106e-01, -6.4772e-02,\n",
       "        -1.1045e+00, -5.1505e-01,  4.2414e-01, -7.2865e-01, -3.4374e-03,\n",
       "         3.7173e-01, -2.9423e-01, -8.9399e-01, -4.6640e-01,  5.8632e-01,\n",
       "         7.6490e-01, -4.6278e-03, -2.1184e+00,  1.4308e+00, -4.4599e-01,\n",
       "         5.9064e-01,  3.1366e-01,  4.0188e-01, -2.2940e-01,  4.6892e-01,\n",
       "        -7.1450e-01,  1.4685e+00, -4.8104e-01,  5.0663e-01,  4.1119e-01,\n",
       "        -5.9042e-01,  1.2643e+00,  3.3558e-01,  2.0767e+00, -1.4490e-01,\n",
       "         6.0952e-01, -2.7853e-01, -3.5707e-01, -1.3044e-01,  7.8752e-01,\n",
       "         1.5287e-01,  8.1577e-01,  1.1871e+00, -8.7495e-01,  1.6764e-01,\n",
       "         6.8943e-01, -4.0276e-01, -5.5285e-01, -8.3414e-02,  3.6776e-01,\n",
       "         7.8726e-01,  1.1056e+00,  1.2769e+00, -2.6139e-01,  5.1668e-01,\n",
       "         3.2391e-01, -2.4290e-01,  1.8376e+00,  9.2985e-01, -5.7956e-01,\n",
       "         8.8111e-01, -1.2066e+00, -1.0062e+00,  4.0748e-01,  1.0542e+00,\n",
       "        -1.6639e-01, -1.2070e+00, -5.1312e-01,  1.5379e+00,  4.9551e-01,\n",
       "        -3.4783e-01,  3.9584e-01,  9.1910e-01, -4.2897e-01,  5.2054e-01,\n",
       "         1.4778e-01, -6.8629e-01,  5.7642e-01, -9.0034e-01,  4.5269e-01,\n",
       "         1.4974e+00,  1.4051e-01, -1.0886e+00, -6.0852e-01, -3.6148e-01,\n",
       "         8.2130e-01, -6.7789e-01,  9.0465e-01, -7.8431e-01, -5.6130e-02,\n",
       "         2.1166e-01, -5.5539e-01, -1.0269e+00, -4.1286e-01, -1.9075e-01,\n",
       "        -7.9466e-01,  2.7922e-01,  2.2589e-01, -8.3352e-02,  2.5753e-01,\n",
       "         3.1823e-01,  1.0595e+00, -7.1391e-02, -1.0049e+00, -7.1956e-01,\n",
       "         6.0130e-01,  3.7373e-01,  7.4507e-01,  4.6285e-01,  3.3278e-01,\n",
       "         1.3037e+00,  5.9571e-01, -3.9055e-01,  9.6921e-01, -3.3561e-01,\n",
       "        -1.2919e-01, -4.8012e-01, -6.5225e-02,  6.3401e-01, -1.0178e+00,\n",
       "        -5.4130e-01, -1.6752e-01, -4.6402e-01, -4.8280e-01, -4.9992e-01,\n",
       "        -1.7454e-01,  2.4091e-01, -1.2157e+00, -3.9383e-01,  6.1886e-02,\n",
       "        -9.9606e-01,  1.3619e-01, -5.5797e-01,  1.6180e+00, -3.3295e-01,\n",
       "         5.9683e-01, -1.0914e+00,  1.1135e+00,  5.3283e-01, -1.0454e+00,\n",
       "         4.6845e-02,  5.1934e-01, -7.3791e-01,  2.0836e-01,  2.8691e-01,\n",
       "         9.5469e-01,  4.2258e-01, -4.5498e-02, -3.9990e-01, -2.8925e-01,\n",
       "        -1.1167e-01,  1.5799e+00,  4.3272e-01, -1.6138e-01,  7.1632e-01,\n",
       "         1.8399e-01,  2.2595e-02,  3.9760e-02,  9.3614e-01,  2.9414e-01,\n",
       "         4.2095e-01,  8.6573e-01,  5.2161e-01, -2.9643e-01, -6.5189e-01,\n",
       "        -6.3346e-02,  4.3692e-01, -9.4993e-01,  1.2721e-01,  1.0395e-03,\n",
       "         2.3661e-01,  4.3850e-01, -1.9579e-01,  2.4927e-01, -1.0406e+00,\n",
       "        -1.2107e+00,  1.4339e+00, -1.9160e+00, -3.5009e-01,  5.1407e-01,\n",
       "        -4.9598e-01,  4.2482e-01,  1.4337e-01, -5.6154e-02, -1.5702e+00,\n",
       "         6.4677e-01, -7.3619e-01, -5.7728e-01, -1.7713e-01,  1.2839e+00,\n",
       "        -5.1122e-01,  2.3254e+00,  8.1387e-01,  4.1720e-01, -7.4411e-01,\n",
       "        -3.6740e-01,  1.2396e-01,  2.7631e-01,  9.6038e-01,  1.1829e-01,\n",
       "        -4.4665e-02, -6.5240e-01, -3.9956e-01, -1.2930e+00,  6.3223e-01,\n",
       "        -1.5878e-01,  2.2683e-01,  1.3444e+00,  7.3260e-01, -5.5976e-01,\n",
       "         6.0070e-01, -5.7417e-01,  5.9123e+00,  2.6837e-01,  6.7097e-01,\n",
       "         2.7267e-01,  1.8426e+00, -8.7313e-01, -4.3917e-01,  5.1889e-01,\n",
       "        -1.3471e-01,  1.2169e+00, -4.2697e-01, -6.8138e-01,  1.0460e+00,\n",
       "         1.2836e+00,  7.1416e-01,  3.6399e-01, -6.5365e-01,  8.1760e-01,\n",
       "         2.2055e-01, -8.0480e-01, -1.2722e+00, -2.3044e-01,  2.4111e-01,\n",
       "         8.1684e-01, -8.9800e-01, -9.0239e-01, -2.2295e-01, -7.2050e-01,\n",
       "         2.0859e+00, -2.0896e-01,  5.9205e-01, -1.3838e-01,  2.1031e-01,\n",
       "        -4.1791e-01, -4.2693e-01,  9.2887e-01, -1.0340e+00,  7.7507e-01,\n",
       "        -1.2488e+00, -6.7907e-01, -1.1058e-01,  1.5521e+00,  1.4428e+00,\n",
       "         7.7948e-01,  1.4771e+00,  1.0450e+00, -1.1399e-01, -1.9314e-01,\n",
       "         6.6327e-01, -2.1502e+00, -5.9605e-01, -3.7461e-01,  1.3185e+00,\n",
       "        -4.0539e-01, -1.9924e-01, -1.2398e-01, -1.7934e-01,  2.8710e-01,\n",
       "         2.4711e+00,  2.4972e-01, -8.5530e-01, -9.4722e-02,  1.8852e-02,\n",
       "        -5.9696e-01,  1.5190e-01, -1.0015e+00,  5.8733e-01,  1.6712e-01,\n",
       "        -5.2990e-01, -1.7305e+00,  9.6499e-01, -6.5814e-01, -1.1925e-01,\n",
       "         7.4712e-01, -1.1927e+00, -7.8051e-03,  1.3898e+00, -8.4688e-01,\n",
       "         5.8653e-01,  1.8810e-01, -3.5123e-01, -7.9608e-01,  3.6480e-01,\n",
       "         7.2382e-01, -3.8982e-01,  1.8343e-02,  4.6892e-01,  6.4048e-01,\n",
       "         7.8372e-01,  4.3027e-01,  2.7689e-01, -6.5473e-01, -2.7292e-02,\n",
       "        -1.2638e+00, -1.1454e+00,  3.5214e-01,  7.2316e-02, -6.8970e-01,\n",
       "        -1.1597e+00, -1.0497e-01, -2.3343e-01, -3.7433e-03,  3.8286e-01,\n",
       "        -3.9913e-01, -7.5219e-01,  1.2959e+00, -1.7800e+00, -2.3655e+00,\n",
       "        -1.5391e-01,  3.4887e-02, -1.8541e-01, -7.0596e-02,  2.1439e-01,\n",
       "        -7.1201e-01, -3.3378e-01, -6.9030e-01, -6.2300e-01,  1.9138e-01,\n",
       "         1.9161e-01, -1.0103e-01, -2.6069e-01,  1.0229e+00, -5.6399e-01,\n",
       "        -4.3757e-01,  4.1649e-01,  1.6034e+00,  2.9888e-01,  1.5624e-01,\n",
       "         1.3945e+00,  7.3216e-01,  5.9324e-01, -1.6092e+00,  3.6906e-01,\n",
       "        -1.7511e+00,  1.3315e-01, -8.6215e-01, -3.4004e-02,  1.6499e+00,\n",
       "        -6.1118e-01, -5.6063e-01, -3.1831e-01,  5.9641e-01, -6.6024e-01,\n",
       "         6.9147e-02,  1.0855e+00, -1.2995e+00, -4.7118e-01, -8.0423e-01,\n",
       "        -9.5970e-01, -2.6920e-01, -7.3290e-01, -6.7173e-01,  1.3442e-01,\n",
       "        -7.1010e-01, -2.1092e+00, -2.0341e-01, -3.6063e-01, -7.6387e-01,\n",
       "        -2.1443e-01, -1.8974e-01,  1.3012e+00, -3.2651e-01, -9.4760e-01,\n",
       "        -4.4889e-01, -7.4412e-01,  2.0072e-01,  1.1853e+00,  5.4856e-02,\n",
       "        -5.8675e-01, -4.5872e-01, -7.5763e-01,  7.2466e-01,  1.4712e+00,\n",
       "        -3.5469e-03,  1.2919e+00, -1.0103e+00, -3.0954e-01, -7.4782e-01,\n",
       "         1.9637e-01, -7.4679e-01,  1.2950e-01, -3.0627e-01, -2.2170e-01,\n",
       "         5.8622e-01,  2.9382e-01,  1.2733e+00, -1.0995e+00, -2.6729e-01,\n",
       "         9.4967e-01, -4.7610e-01,  9.5941e-01, -3.8638e-01,  3.6385e-01,\n",
       "        -1.5944e+00,  4.8035e-01, -1.5912e-01,  1.1880e+00, -6.1456e-01,\n",
       "        -2.3055e-01, -1.9043e-01, -3.9331e-01,  5.0683e-01,  1.0649e+00,\n",
       "         4.8333e-01, -8.9671e-01,  1.5282e+00, -3.7002e-01, -1.0229e+00,\n",
       "         2.9107e-01,  1.5957e-01, -1.8386e+00, -7.5830e-01, -6.8063e-01,\n",
       "         6.3864e-03, -2.0318e-01,  7.2828e-01,  1.9769e+00,  2.2016e+00,\n",
       "        -9.0410e-01,  7.6020e-01,  3.5069e-01,  8.6767e-02,  4.3376e-01,\n",
       "        -5.6239e-01, -7.8224e-02, -3.1343e-01,  9.4118e-02,  3.6508e-02,\n",
       "        -4.5706e-01, -5.4238e-01,  4.5241e-01, -1.2566e-01,  1.1237e+00,\n",
       "         2.8171e-01,  7.6789e-01, -7.2641e-01,  5.8711e-01,  7.0131e-01,\n",
       "         9.7802e-03, -2.1756e+00,  2.0379e-01,  1.6813e-01, -1.3762e+00,\n",
       "         1.2926e-01, -2.4880e-01, -3.1390e-01, -8.2493e-01, -9.1288e-01,\n",
       "        -1.8948e+00,  5.1283e-01, -1.0462e-01,  3.0812e-01,  7.0683e-01,\n",
       "        -2.6443e-01, -2.3841e-01, -1.0694e-01,  1.7419e+00,  1.4551e+00,\n",
       "        -3.3884e-01, -9.4694e-01,  7.9599e-01, -8.3456e-01, -5.2035e-01,\n",
       "         1.7535e+00, -4.1488e+00,  3.9586e-01, -1.1505e-01,  5.3734e-01,\n",
       "         5.6809e-02,  5.5074e-01, -5.4575e-01, -2.5275e-01,  2.1714e-01,\n",
       "         1.7485e-01,  2.6060e-01,  1.0357e-01, -1.6651e-01,  3.0361e-01,\n",
       "        -7.3609e-01, -3.0139e-02, -4.5965e-01], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_embedding = output.last_hidden_state[0, 0, :]\n",
    "cls_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Layer (Linear/Dense/Fully Connected)\n",
    "\n",
    "on top of BERT\n",
    "untrained pa eto kaya random pa ang weights\n",
    "\n",
    "-> ang output ay *logits* (raw score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3199, -0.4695, -0.0829,  0.2421,  0.0986, -0.7168],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = nn.Linear(768, 6)  # 6 labels\n",
    "logits = classifier(cls_embedding)\n",
    "logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid Activation\n",
    "para maconvert ang logits as a number between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4207, 0.3847, 0.4793, 0.5602, 0.5246, 0.3281],\n",
      "       grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "probabilities = torch.sigmoid(logits)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply threshold = 0.5\n",
    "\n",
    "Probabilities greater than 0.5 means the label exists and below means label does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 1., 0.])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "y_pred = np.zeros(probabilities.shape)\n",
    "y_pred[np.where(probabilities >= threshold)] = 1\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: ['Race', 'Religion']\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = [label for label, pred in zip(LABELS, y_pred) if pred == 1]\n",
    "print(\"Predicted labels:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hamming Loss per instance\n",
    "\n",
    "Fraction of wrong labels to all labels\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "          number of incorrect labels\n",
    "    HL = -----------------------------\n",
    "           total number of labels\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 0. 0.]\n",
      "True labels: ['Gender']\n"
     ]
    }
   ],
   "source": [
    "# get true labels\n",
    "labels = representation['labels'] # [0, 1, 0, 0, 0, 0]\n",
    "y_true = np.zeros(labels.shape)\n",
    "y_true[np.where(labels >= threshold)] = 1\n",
    "print(y_true)\n",
    "true_labels = [label for label, true_label in zip(LABELS, y_true) if true_label == 1]\n",
    "print(\"True labels:\", true_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 1. 0.]\n",
      "[False  True False  True  True False] (True means label is misclassified, False if correct label)\n",
      "3 wrong labels\n",
      "Hamming Loss: 0.5\n"
     ]
    }
   ],
   "source": [
    "print(y_true)\n",
    "print(y_pred)\n",
    "\n",
    "xor_result = np.logical_xor(y_true, y_pred)\n",
    "\n",
    "print(xor_result, \"(True means label is misclassified, False if correct label)\")\n",
    "xor_sum = np.sum(xor_result)\n",
    "print(xor_sum, \"wrong labels\" )\n",
    "\n",
    "total_labels = len(y_true)\n",
    "hamming_loss = xor_sum / total_labels\n",
    "print(\"Hamming Loss:\", hamming_loss)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

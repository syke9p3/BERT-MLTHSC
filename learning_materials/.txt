# For mass encoding

encoded_dataset = []

# Iterate through each row in the original dataset
for _, data_row in df.iterrows():
    hate_speech_text = data_row['Text']
    labels = data_row[LABELS]

    encoding = BERT_TOKENIZER.encode_plus(
        hate_speech_text,
        add_special_tokens=True,
        return_tensors='pt',
        padding='max_length',
        truncation=True,
        max_length=128,
        return_token_type_ids=False,
        return_attention_mask=True
    )

    representation = {
        'input_ids': encoding['input_ids'].flatten(),
        'attention_mask': encoding['attention_mask'].flatten(),
        'labels': torch.FloatTensor(labels)
    }

    # Append the encoded representation to the list
    encoded_dataset.append(representation)


# Convert the list of dictionaries to a DataFrame
encoded_df = pd.DataFrame(encoded_dataset)

